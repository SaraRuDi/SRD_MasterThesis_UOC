{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for general plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#library for dataset\n",
    "from ucimlrepo import fetch_ucirepo  \n",
    "\n",
    "# Download dataset from UCI ML repository\n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
    "\n",
    "# Divide database in two pandas dataframes \n",
    "X_base = cdc_diabetes_health_indicators.data.features\n",
    "y_base = cdc_diabetes_health_indicators.data.targets\n",
    "\n",
    "\n",
    "# VERIFY NO MISSING VALUES\n",
    "print(X_base.info())\n",
    "print(y_base.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the type of values in each feature\n",
    "for column in X_base.columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(X_base[column].unique())\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"Target Column: Diabetes_binary\")\n",
    "print(y_base['Diabetes_binary'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of people with/without diabetes\n",
    "conteo = y_base['Diabetes_binary'].value_counts()\n",
    "\n",
    "print(conteo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling to balance diabetic and non-diabetic population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine X and y for consistent resampling\n",
    "data = X_base.copy()\n",
    "data['Diabetes_binary'] = y_base\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority = data[data['Diabetes_binary'] == 0]\n",
    "minority = data[data['Diabetes_binary'] == 1]\n",
    "\n",
    "# Undersample majority class\n",
    "majority_undersampled = resample(majority,\n",
    "                                 replace=False,  # Sample without replacement\n",
    "                                 n_samples=len(minority),  # Match the minority class size\n",
    "                                 random_state=42)  # For reproducibility\n",
    "\n",
    "# Combine minority and undersampled majority class\n",
    "balanced_data = pd.concat([majority_undersampled, minority])\n",
    "\n",
    "# Shuffle the resulting dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split back into X and y\n",
    "X = balanced_data.drop(columns=['Diabetes_binary'])\n",
    "y = balanced_data['Diabetes_binary']\n",
    "X_original = X.copy()\n",
    "# Check the class distribution\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Standarization for numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to standardize (continuous features BMI, MentHlth, and PhysHlth)\n",
    "columns_to_standardize = ['BMI', 'MentHlth', 'PhysHlth']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale only the selected columns\n",
    "scaled_values = scaler.fit_transform(X[columns_to_standardize])  # Returns ndarray\n",
    "X[columns_to_standardize] = scaled_values\n",
    "print(X[columns_to_standardize].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output confirm that the standardization process was applied successfully. The mean values for each standardized featureare very close to zero, aligned with the expected outcome. The standard deviation values are approximately one for each feature, indicating that the spread of data has been adjusted to a unit scale, ensuring consistent weighting across variables. The range of values also reflects a standardized distribution, with BMI spanning from approximately -2.48 to 10.53, while MentHlth and PhysHlth show a similarly adjusted range. These results confirm that the selected features have been appropriately scaled, balancing their influence for clustering and dimensionality reduction and preparing the dataset for effective segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical values\n",
    "\n",
    "Dataset already encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary, nominal, and ordinal columns\n",
    "binary_columns = ['HighBP', 'HighChol', 'CholCheck','Smoker', 'Stroke', 'PhysActivity', 'Fruits', 'Veggies', \n",
    "                  'HeartDiseaseorAttack', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'DiffWalk', 'Sex']\n",
    "\n",
    "ordinal_columns = ['Education', 'Income', 'Age', 'GenHlth'] \n",
    "numerical_columns = ['BMI', 'MentHlth', 'PhysHlth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    " Variance Thresholding is used to eliminate features with low variance, as they provide limited information about the differences between data points. Correlation Analysis helps identify and remove highly correlated features, reducing redundancy without sacrificing information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold (0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Set a threshold for variance (e.g., 0.01)\n",
    "selector = VarianceThreshold(threshold=0.05)\n",
    "X_var = selector.fit_transform(X)\n",
    "print(f\"Number of features after variance thresholding: {X_var.shape[1]}\")\n",
    "# Use the `get_support` method to get a boolean mask of selected features\n",
    "selected_features_mask = selector.get_support()\n",
    "\n",
    "# Get the column names corresponding to the selected features\n",
    "selected_features = X.columns[selected_features_mask]\n",
    "non_selected_features = X.columns[~selected_features_mask]  # Invert the mask to select non-selected features\n",
    "\n",
    "# Print the names of the non-selected features\n",
    "print(\"Non-selected features after variance thresholding:\", non_selected_features.tolist())\n",
    "# Print the names of the features selected after variance thresholding\n",
    "print(\"Selected features after variance thresholding:\", selected_features.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop low variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X.drop(columns=['CholCheck', 'HvyAlcoholConsump', 'AnyHealthcare'])\n",
    "binary_columns = ['HighBP', 'HighChol', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'NoDocbcCost', 'GenHlth',\n",
    "        'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for highly correlated columns (>0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "# Find correlated features with correlation higher than a threshold of 0.8\n",
    "high_corr_var = [column for column in corr_matrix.columns if any(corr_matrix[column] > 0.8)]\n",
    "print(\"Highly correlated features:\", high_corr_var)\n",
    "\n",
    "# Drop highly correlated features\n",
    "X_sel = X.drop(columns=high_corr_var)\n",
    "\n",
    "#Plot matrix\n",
    "plt.figure(figsize=(12, 10))  \n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying correlation analysis to the dataset, no features were removed due to high correlation. This outcome indicates that none of the retained features exhibited correlation levels exceeding the threshold (0.8), suggesting that the remaining features are largely independent of one another. The lack of high correlation between features implies minimal redundancy, as each feature contributes unique information to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm implementation (Clustering)\n",
    "\n",
    "To identify distinct patient segments within the dataset, various clustering algorithms are implemented and evaluated. Clustering is a core unsupervised machine learning technique that groups data points based on their similarity, providing insight into underlying patterns without predefined labels. Given the complexity of the dataset, which includes a range of demographic, health, and lifestyle indicators, multiple clustering methods are bieng employed to capture different aspects of the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Prototypes: 2, 3 and 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for K-Prototypes\n",
    "X_np = X.to_numpy()\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for gamma: gamma_values = np.arange(0.5, 5.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "numerical_columns = ['BMI', 'MentHlth', 'PhysHlth']\n",
    "categorical_columns_indices = list(set(X.columns) - set(numerical_columns))\n",
    "\n",
    "# Convert categorical column names to indices\n",
    "categorical_indices = [X.columns.get_loc(col) for col in categorical_columns_indices]\n",
    "\n",
    "# Range of gamma values to try\n",
    "gamma_values = np.arange(0.5, 5.5, 1)  # Try gamma from 0.5 to 5.0\n",
    "\n",
    "best_gamma = None\n",
    "best_score = -1\n",
    "best_kproto = None\n",
    "\n",
    "for gamma in gamma_values:\n",
    "\n",
    "    kproto = KPrototypes(n_clusters=2, init='Huang', verbose=1, random_state=42, gamma=gamma)\n",
    "    clusters = kproto.fit_predict(X_np, categorical=categorical_indices)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    silhouette = silhouette_score(X_np, clusters, metric='hamming')\n",
    "    dbi = davies_bouldin_score(X_np, clusters)\n",
    "    chi = calinski_harabasz_score(X_np, clusters)\n",
    "\n",
    "    # Print results for each gamma\n",
    "    print(f\"\\nGamma: {gamma:.2f}, Clusters: 2\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "    print(f\"Davies-Bouldin Index (DBI): {dbi:.3f}\")\n",
    "    print(f\"Calinski-Harabasz Index (CHI): {chi:.3f}\")\n",
    "\n",
    "    # Track the best gamma based on silhouette score\n",
    "    if silhouette > best_score:\n",
    "        best_score = silhouette\n",
    "        best_gamma = gamma\n",
    "        best_kproto = kproto\n",
    "\n",
    "print(f\"\\nBest Gamma: {best_gamma:.2f} with Silhouette Score: {best_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize numerical features for ROCK and Genetic algorithms\n",
    "Binning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_original\n",
    "X.drop(columns=['CholCheck', 'HvyAlcoholConsump', 'AnyHealthcare'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BMI bins and labels\n",
    "bmi_bins = [0, 18.5, 25, 30, 35, 40, np.inf]\n",
    "bmi_labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Apply binning to BMI\n",
    "X['BMI_Category'] = pd.cut(X['BMI'], bins=bmi_bins, labels=bmi_labels, right=False)\n",
    "\n",
    "# Define MentHlth and PhysHlth bins and labels\n",
    "health_bins = [0, 11, 21, 31]\n",
    "health_labels = [0, 1, 2]\n",
    "\n",
    "# Apply binning to MentHlth and PhysHlth\n",
    "X['MentHlth_Category'] = pd.cut(X['MentHlth'], bins=health_bins, labels=health_labels, right=False)\n",
    "X['PhysHlth_Category'] = pd.cut(X['PhysHlth'], bins=health_bins, labels=health_labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROCK (Robust Clustering using LinKs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Take a 25% random sample to avoid memory issues\n",
    "X_sampled = X.sample(frac=0.25, random_state=42)  # 25% sample for avoiding memory issues\n",
    "X_sampled.drop(columns=['BMI', 'MentHlth', 'PhysHlth'], inplace=True)\n",
    "print(X_sampled.dtypes)\n",
    "\n",
    "# Convert category columns to integer and drop NaN values\n",
    "X_sampled['BMI_Category'] = X_sampled['BMI_Category'].astype(int)\n",
    "X_sampled['MentHlth_Category'] = X_sampled['MentHlth_Category'].astype(int)\n",
    "X_sampled['PhysHlth_Category'] = X_sampled['PhysHlth_Category'].astype(int)\n",
    "X_sampled.dropna(inplace=True)\n",
    "\n",
    "# Flatten true labels to ensure they are 1D\n",
    "true_labels = y.loc[X_sampled.index].values.ravel()\n",
    "\n",
    "# Define cluster values and theta grid for fine grid search\n",
    "cluster_values = [2, 3, 5]\n",
    "theta_values = np.arange(0.1, 1, 0.1)  #  0.1 to 0.9\n",
    "\n",
    "# Store results \n",
    "results = []\n",
    "\n",
    "# Grid search over theta values\n",
    "for theta in theta_values:\n",
    "    print(f\"\\n--- Evaluating for Theta = {theta:.1f} ---\")\n",
    "    \n",
    "    # Apply thresholding to compute the pairwise Hamming distance matrix\n",
    "    hamming_distance_matrix = pairwise_distances(X_sampled, metric='hamming')\n",
    "\n",
    "    # Apply threshold to similarity \n",
    "    hamming_distance_matrix[hamming_distance_matrix > theta] = 1\n",
    "    hamming_distance_matrix[hamming_distance_matrix <= theta] = 0\n",
    "    \n",
    "    # Ensure diagonal is zero\n",
    "    np.fill_diagonal(hamming_distance_matrix, 0)\n",
    "\n",
    "    # Loop over different cluster values\n",
    "    for n_clusters in cluster_values:\n",
    "        print(f\"\\nEvaluating with {n_clusters} clusters:\")\n",
    "        \n",
    "        # Clustering\n",
    "        clustering_sampled = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, metric='precomputed', linkage='average'\n",
    "        )\n",
    "        clusters_sampled = clustering_sampled.fit_predict(hamming_distance_matrix)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        sil_score = silhouette_score(hamming_distance_matrix, clusters_sampled, metric=\"precomputed\")\n",
    "        chi_score = calinski_harabasz_score(X_sampled, clusters_sampled)\n",
    "        dbi_score = davies_bouldin_score(X_sampled, clusters_sampled)\n",
    "        ari_score = adjusted_rand_score(true_labels, clusters_sampled)\n",
    "\n",
    "        # Print the evaluation metrics\n",
    "        print(f\"Theta: {theta:.1f}, Clusters: {n_clusters}\")\n",
    "        print(f\"Silhouette Score: {sil_score:.4f}\")\n",
    "        print(f\"Calinski-Harabasz Index: {chi_score:.4f}\")\n",
    "        print(f\"Davies-Bouldin Index: {dbi_score:.4f}\")\n",
    "\n",
    "        # Store results in a list for later analysis\n",
    "        results.append({\n",
    "            'theta': theta,\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette_score': sil_score,\n",
    "            'calinski_harabasz_score': chi_score,\n",
    "            'davies_bouldin_score': dbi_score\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for further analysis or visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Grid Search Results ---\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Based on Genetic Algorithms\n",
    "Genetic algorithms for clustering typically involve using an evolutionary process to optimize cluster assignments. There is no direct implementation in Scikit-learn, but DEAP (Distributed Evolutionary Algorithms in Python) can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "# Columns info\n",
    "total_columns = X.columns\n",
    "categorical_indices = list(range(X.shape[1]))\n",
    "\n",
    "# Mixed distance function for categorical data\n",
    "def categorical_distance(data, centroid):\n",
    "    return np.sum(data != centroid, axis=1) / data.shape[1]\n",
    "\n",
    "# Calculate cluster assignment based on nearest centroids using Hamming distance\n",
    "def assign_clusters(centroids, data):\n",
    "    distances = np.array([\n",
    "        categorical_distance(data, centroid)\n",
    "        for centroid in centroids\n",
    "    ]).T\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "# minimize intra-cluster distance\n",
    "def evaluate_categorical(individual, data, n_clusters):\n",
    "    centroids = np.array(individual).reshape(n_clusters, -1)\n",
    "    clusters = assign_clusters(centroids, data)\n",
    "    intra_cluster_dist = np.sum([\n",
    "        categorical_distance(np.expand_dims(data[i], axis=0), np.expand_dims(centroids[clusters[i]], axis=0))[0]\n",
    "        for i in range(data.shape[0])\n",
    "    ])\n",
    "    penalty = 1000 if len(np.unique(clusters)) != n_clusters else 0\n",
    "    return intra_cluster_dist + penalty,\n",
    "\n",
    "# Genetic algorithm for clustering with early stopping\n",
    "def genetic_clustering(data, n_clusters, n_gen=30, pop_size=20, cxpb=0.7, mutpb=0.2, patience=5):\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    if \"FitnessMin\" not in creator.__dict__:\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    if \"Individual\" not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_cat\", random.choice, np.unique(data))\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_cat, n_clusters * n_features)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "    toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evaluate_categorical, data=data, n_clusters=n_clusters)\n",
    "\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "    start_time = time.time()\n",
    "    evals = []\n",
    "\n",
    "    for gen in range(n_gen):\n",
    "        algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=1, verbose=False)\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        evals.append(best_ind.fitness.values[0])\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Generation {gen+1}/{n_gen} - Best Fitness: {evals[-1]} - Elapsed Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        if len(evals) > patience and np.abs(evals[-1] - evals[-patience]) < 1e-3:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    best_centroids = np.array(best_ind).reshape(n_clusters, -1)\n",
    "\n",
    "    clusters = assign_clusters(best_centroids, data)\n",
    "    \n",
    "    unique_clusters = np.unique(clusters)\n",
    "    if len(unique_clusters) <= 1 or len(unique_clusters) >= len(data):\n",
    "        silhouette, dbi, chi = -1, -1, -1\n",
    "    else:\n",
    "        silhouette = silhouette_score(data, clusters, metric='hamming')\n",
    "        dbi = davies_bouldin_score(data, clusters)\n",
    "        chi = calinski_harabasz_score(data, clusters)\n",
    "    \n",
    "    return clusters, best_centroids, silhouette, dbi, chi\n",
    "\n",
    "# Hyperparameter grid search\n",
    "pop_sizes = [20, 30, 50]\n",
    "cxpbs = [0.6, 0.7, 0.8]\n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for pop_size, cxpb in product(pop_sizes, cxpbs):\n",
    "    print(f\"Testing GA with pop_size={pop_size}, cxpb={cxpb}\")\n",
    "    for n_clusters in [2]:\n",
    "        clusters, centroids, sil_score, dbi, chi = genetic_clustering(\n",
    "            X.values, n_clusters, pop_size=pop_size, cxpb=cxpb\n",
    "        )\n",
    "        best_results[(n_clusters, pop_size, cxpb)] = {\n",
    "            'clusters': clusters,\n",
    "            'centroids': centroids,\n",
    "            'silhouette_score': sil_score,\n",
    "            'dbi': dbi,\n",
    "            'chi': chi\n",
    "        }\n",
    "\n",
    "# Print best results\n",
    "for k, v in best_results.items():\n",
    "    print(f\"\\nClusters: {k[0]}, Pop Size: {k[1]}, Generations: {k[2]}, cxpb: {k[3]}, mutpb: {k[4]}\")\n",
    "    print(f\"Silhouette Score: {v['silhouette_score']:.4f}\")\n",
    "    print(f\"Davies-Bouldin Index: {v['dbi']:.4f}\")\n",
    "    print(f\"Calinski-Harabasz Index: {v['chi']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability analysis for best performing algorithm: ROCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# parameters to study stability for each cluster configuration\n",
    "best_parameters = {\n",
    "    2: 0.6,  # theta for 2 clusters \n",
    "    3: 0.6,  # theta for 3 clusters\n",
    "    5: 0.5   # theta for 5 clusters\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "stability_results = []\n",
    "\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "for n_clusters, theta in best_parameters.items():\n",
    "    print(f\"\\nEvaluating stability for {n_clusters} clusters with theta={theta:.1f}\")\n",
    "\n",
    "    cluster_assignments = []  # To store cluster assignments for each fold\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_sampled):\n",
    "        X_train, X_test = X_sampled.iloc[train_index], X_sampled.iloc[test_index]\n",
    "\n",
    "        # Compute Hamming distance matrix for the training data\n",
    "        hamming_distance_matrix = pairwise_distances(X_train, metric='hamming')\n",
    "        hamming_distance_matrix[hamming_distance_matrix > theta] = 1\n",
    "        hamming_distance_matrix[hamming_distance_matrix <= theta] = 0\n",
    "        np.fill_diagonal(hamming_distance_matrix, 0)\n",
    "        \n",
    "        # Apply Agglomerative Clustering on training data\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, metric='precomputed', linkage='average'\n",
    "        )\n",
    "        clusters = clustering.fit_predict(hamming_distance_matrix)\n",
    "        \n",
    "        # Store cluster assignments\n",
    "        cluster_assignments.append(pd.Series(clusters, index=X_train.index))\n",
    "    \n",
    "    # Compare cluster stability across folds using Spearman's rank correlation\n",
    "    pairwise_correlations = []\n",
    "    for i in range(len(cluster_assignments)):\n",
    "        for j in range(i + 1, len(cluster_assignments)):\n",
    "            common_indices = cluster_assignments[i].index.intersection(cluster_assignments[j].index)\n",
    "            if len(common_indices) > 0:\n",
    "                corr, _ = spearmanr(\n",
    "                    cluster_assignments[i].loc[common_indices],\n",
    "                    cluster_assignments[j].loc[common_indices]\n",
    "                )\n",
    "                pairwise_correlations.append(corr)\n",
    "\n",
    "    # Calculate mean stability for this cluster configuration\n",
    "    mean_stability = np.mean(pairwise_correlations) if pairwise_correlations else np.nan\n",
    "    print(f\"Mean Stability for {n_clusters} clusters with theta={theta:.1f}: {mean_stability:.4f}\")\n",
    "    \n",
    "    # Append to results\n",
    "    stability_results.append({\n",
    "        'n_clusters': n_clusters,\n",
    "        'theta': theta,\n",
    "        'mean_stability': mean_stability\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for further analysis or visualization\n",
    "stability_results_df = pd.DataFrame(stability_results)\n",
    "print(\"\\n--- Cluster Stability Results ---\")\n",
    "print(stability_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cluster labels for 2 clusters\n",
    "clustering = AgglomerativeClustering(n_clusters=2, metric='precomputed', linkage='average')\n",
    "labels = clustering.fit_predict(hamming_distance_matrix)\n",
    "print(\"Cluster labels for 2 clusters:\", np.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(kf.split(hamming_distance_matrix)):\n",
    "    train_dist = hamming_distance_matrix[train_idx][:, train_idx]\n",
    "    clustering = AgglomerativeClustering(n_clusters=2, metric='precomputed', linkage='average')\n",
    "    train_labels = clustering.fit_predict(train_dist)\n",
    "    print(f\"Fold {fold + 1}: Cluster counts - {np.bincount(train_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicability and interpretability of the clustering\n",
    "- SHAP for the global understanding of the feature importance in cluster forming. \n",
    "- LIME to obtain detailed explications (study atypic cases or miss defined clusters) \n",
    "- Finally a decision tree to generate clear rules which are visually interpretable, easing the communication of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use the final configuration for cluster assignments\n",
    "optimal_theta = 0.6\n",
    "optimal_clusters = 2  # best number of clusters\n",
    "final_hamming_distance_matrix = pairwise_distances(X_sampled, metric='hamming')\n",
    "final_hamming_distance_matrix[final_hamming_distance_matrix > optimal_theta] = 1\n",
    "final_hamming_distance_matrix[final_hamming_distance_matrix <= optimal_theta] = 0\n",
    "np.fill_diagonal(final_hamming_distance_matrix, 0)\n",
    "\n",
    "final_clustering = AgglomerativeClustering(\n",
    "    n_clusters=optimal_clusters, metric='precomputed', linkage='average'\n",
    ")\n",
    "final_clusters = final_clustering.fit_predict(final_hamming_distance_matrix)\n",
    "\n",
    "# Train a surrogate model to predict final cluster labels\n",
    "X_features = X_sampled.copy()\n",
    "X_features['cluster_label'] = final_clusters  # Add cluster labels\n",
    "\n",
    "# Define features and labels for surrogate model\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features.drop(columns=['cluster_label']), \n",
    "    X_features['cluster_label'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the surrogate model\n",
    "surrogate_model = RandomForestClassifier(random_state=42)\n",
    "surrogate_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate surrogate model performance\n",
    "y_pred = surrogate_model.predict(X_test)\n",
    "print(\"\\n--- Surrogate Model Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Apply SHAP to explain cluster assignments\n",
    "explainer = shap.Explainer(surrogate_model, X_train)\n",
    "shap_values = explainer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of shap_values:\", shap_values.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_cluster0 = shap_values[:, :, 0]  # Select SHAP values for Cluster 0\n",
    "shap.summary_plot(shap_values_cluster0, X_test)  # Summary plot for Cluster 0\n",
    "\n",
    "shap_values_avg = shap_values.mean(axis=2)  # Average SHAP values\n",
    "shap.summary_plot(shap_values_avg, X_test)  # Summary plot for averaged SHAP values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['original_cluster_label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shap_values_cluster0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of shap_values:\", shap_values.values.shape)  # Should be (n_samples, n_features)\n",
    "print(\"Shape of X_test:\", X_test.shape)  # Number of features must match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the final configuration for cluster assignments\n",
    "optimal_theta = 0.6\n",
    "optimal_clusters = 2\n",
    "final_hamming_distance_matrix = pairwise_distances(X_sampled, metric='hamming')\n",
    "final_hamming_distance_matrix[final_hamming_distance_matrix > optimal_theta] = 1\n",
    "final_hamming_distance_matrix[final_hamming_distance_matrix <= optimal_theta] = 0\n",
    "np.fill_diagonal(final_hamming_distance_matrix, 0)\n",
    "\n",
    "final_clustering = AgglomerativeClustering(\n",
    "    n_clusters=optimal_clusters, metric='precomputed', linkage='average'\n",
    ")\n",
    "final_clusters = final_clustering.fit_predict(final_hamming_distance_matrix)\n",
    "\n",
    "# Train-Test Split\n",
    "X_features = X_sampled.copy()\n",
    "X_features['cluster_label'] = final_clusters\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features.drop(columns=['cluster_label']), \n",
    "    X_features['cluster_label'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the surrogate model\n",
    "surrogate_model = RandomForestClassifier(random_state=42)\n",
    "surrogate_model.fit(X_train, y_train)\n",
    "\n",
    "# Select representative samples\n",
    "test_clusters = pd.Series(final_clusters, index=X_sampled.index).loc[X_test.index]\n",
    "\n",
    "representative_samples = pd.concat([\n",
    "    X_test[test_clusters == 0].sample(n=1, random_state=42),  # Cluster 0\n",
    "    X_test[test_clusters == 1].sample(n=1, random_state=42)   # Cluster 1\n",
    "])\n",
    "print(\"\\nRepresentative Samples:\")\n",
    "print(representative_samples)\n",
    "\n",
    "# Select uncertain samples\n",
    "probabilities = surrogate_model.predict_proba(X_test)\n",
    "uncertainty_indices = np.argsort(np.abs(probabilities[:, 0] - 0.5))[:5]  # Top 5 uncertain samples\n",
    "uncertain_samples = X_test.iloc[uncertainty_indices]\n",
    "print(\"\\nUncertain Samples:\")\n",
    "print(uncertain_samples)\n",
    "\n",
    "# Select outliers\n",
    "cluster_centers = X_test.groupby(test_clusters).mean()  # Approximate cluster centers\n",
    "distances = cdist(X_test, cluster_centers, metric='hamming')\n",
    "outliers = X_test.iloc[np.argmax(distances, axis=0)]  # One outlier per cluster\n",
    "print(\"\\nOutlier Samples:\")\n",
    "print(outliers)\n",
    "\n",
    "# Combine all selected samples\n",
    "all_selected_samples = pd.concat([representative_samples, uncertain_samples, outliers]).drop_duplicates()\n",
    "print(\"\\nAll Selected Samples for LIME:\")\n",
    "print(all_selected_samples)\n",
    "\n",
    "# Apply LIME\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=np.array(X_train),\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['Cluster 0', 'Cluster 1'],  # For binary clustering\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "for idx, sample in all_selected_samples.iterrows():\n",
    "    print(f\"\\nExplaining sample index {idx}:\\n\", sample)\n",
    "\n",
    "    # Generate LIME explanation\n",
    "    lime_explanation = lime_explainer.explain_instance(\n",
    "        data_row=sample.values,  # Pass the sample as a NumPy array\n",
    "        predict_fn=surrogate_model.predict_proba,  # Surrogate model probabilities\n",
    "        num_features=10  # Top 10 features for explanation\n",
    "    )\n",
    "\n",
    "    # Show explanation\n",
    "    lime_explanation.show_in_notebook()\n",
    "\n",
    "    # Print explanation as text\n",
    "    print(\"\\nLIME Explanation as List:\")\n",
    "    print(lime_explanation.as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in features and target\n",
    "print(X_features.isnull().sum())\n",
    "print(y_target.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_sampled.shape[0], cluster_assignments[-1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the best parameters\n",
    "best_theta = 0.6\n",
    "best_n_clusters = 2\n",
    "\n",
    "# Drop rows with NaN values if any\n",
    "X_sampled.dropna(inplace=True)\n",
    "\n",
    "# Confirm no NaN values remain\n",
    "if X_sampled.isnull().any().any():\n",
    "    print(\"NaN values still present in X_sampled!\")\n",
    "else:\n",
    "    print(\"No NaN values in X_sampled.\")\n",
    "\n",
    "# Recompute the best cluster labels using the best theta\n",
    "hamming_distance_matrix = pairwise_distances(X_sampled, metric='hamming')\n",
    "hamming_distance_matrix[hamming_distance_matrix > best_theta] = 1\n",
    "hamming_distance_matrix[hamming_distance_matrix <= best_theta] = 0\n",
    "np.fill_diagonal(hamming_distance_matrix, 0)\n",
    "\n",
    "clustering_sampled = AgglomerativeClustering(\n",
    "    n_clusters=best_n_clusters, metric='precomputed', linkage='average'\n",
    ")\n",
    "clusters_sampled = clustering_sampled.fit_predict(hamming_distance_matrix)\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "X_sampled['Cluster_Labels'] = clusters_sampled\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_features = X_sampled.drop(columns=['Cluster_Labels'])  # Features\n",
    "y_clusters = X_sampled['Cluster_Labels']                # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_clusters, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree\n",
    "decision_tree = DecisionTreeClassifier(random_state=42, max_depth=5)  # Limit depth for interpretability\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the decision tree\n",
    "train_score = decision_tree.score(X_train, y_train)\n",
    "test_score = decision_tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_score:.2f}\")\n",
    "print(f\"Test Accuracy: {test_score:.2f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(decision_tree, feature_names=X_features.columns, class_names=True, filled=True)\n",
    "plt.title(\"Decision Tree for Clustering\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
